{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install aix360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aix360'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-31b4a8ec7ccc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0maix360\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotodash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProtodashExplainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'aix360'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xgboost\n",
    "import shap\n",
    "import lime\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heloc_data = pd.read_csv('heloc_dataset_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heloc_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heloc_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heloc_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heloc_data.RiskPerformance = heloc_data.RiskPerformance.replace(to_replace = ['Good', 'Bad'], value = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = \n",
    "heloc_data.corr()\n",
    "#sb.heatmap(corr, cmap=\"Blues\", annot=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select import features based on correlation with Y variable (RiskPerformance)\n",
    "\n",
    "heloc_data = heloc_data[[\n",
    "    'NetFractionRevolvingBurden',\n",
    "    'PercentTradesWBalance',\n",
    "    'PercentInstallTrades',\n",
    "    'MaxDelqEver',\n",
    "    'MaxDelq2PublicRecLast12M',\n",
    "    'MSinceMostRecentInqexcl7days',\n",
    "    'PercentTradesNeverDelq',\n",
    "    'NumSatisfactoryTrades',\n",
    "    'MSinceOldestTradeOpen',\n",
    "    'AverageMInFile',\n",
    "    'ExternalRiskEstimate',\n",
    "    'RiskPerformance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X, Y datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heloc_data.drop(columns  = 'RiskPerformance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = heloc_data.RiskPerformance.replace(to_replace = ['Good', 'Bad'], value = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.array(X_train)\n",
    "X_test_array = np.array(X_test)\n",
    "Y_train_array = np.array(Y_train)\n",
    "Y_test_array = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = xgboost.XGBClassifier().fit(X_train_array, Y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = xgboost_model.predict(X_train_array)\n",
    "test_prediction = xgboost_model.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = sklearn.metrics.accuracy_score(Y_train, train_prediction)\n",
    "score_test = sklearn.metrics.accuracy_score(Y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_score, test_score: ', score_train, score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from aix360.algorithms.protodash import ProtodashExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protodash_explain(X_train, X_test, model, idx, m):\n",
    "    \n",
    "# Combine the training data set and the predicted values of the model into the same array, and distinguish the samples whose prediction results are good/bad    \n",
    "    X_train_array = np.array(X_train)\n",
    "    X_test_array = np.array(X_test)\n",
    "    pred_train = model.predict(X_train_array)\n",
    "    pred_train = pred_train.reshape((pred_train.shape[0],1)) \n",
    "    z_train = np.hstack((X_train_array, pred_train)) \n",
    "    z_train_good = z_train[z_train[:,-1]==0, :]\n",
    "    z_train_bad = z_train[z_train[:,-1]==1, :]\n",
    "    \n",
    "    # Randomly select samples to explain from the test dataset\n",
    "    print(\"Chosen Sample:\", idx)\n",
    "    print(\"Prediction made by the model:\", model.predict(X_test_array)[idx])\n",
    "    print(\"Prediction probabilities:\", model.predict_proba(X_test_array[idx].reshape(1,X_test_array.shape[1])))\n",
    "    print(\"\")\n",
    "    Target = np.hstack((X_test_array[idx].reshape((1,11)), model.predict(X_test_array)[idx].reshape((1,1))))\n",
    "    print(\"The values of the sample features to be explained are as follows：\")\n",
    "    print(X_test.iloc[idx])\n",
    "    \n",
    "    # Create Protodash interpreter\n",
    "    explainer = ProtodashExplainer()\n",
    "    (W, S, setValues) = explainer.explain(Target, z_train_good, m) \n",
    "    # Display the value of each prototype feature and the corresponding Weight    \n",
    "    col = X_train.columns.tolist()\n",
    "    col.append('RiskPerformance')\n",
    "    \n",
    "    if model.predict(X_test_array)[idx]==0:\n",
    "        prototypes = pd.DataFrame(z_train_good[S, :])\n",
    "    \n",
    "    else:\n",
    "        prototypes = pd.DataFrame(z_train_bad[S, :])\n",
    "    \n",
    "    prototypes.columns = col\n",
    "    prototypes[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5))\n",
    "    prototypes = prototypes.transpose()\n",
    "    \n",
    "    # Display the Weight of each feature of each prototype\n",
    "    \n",
    "    if model.predict(X_test_array)[idx]==0:\n",
    "        z = z_train_good[S, 0:-1] # Store chosen prototypes\n",
    "    \n",
    "    else:\n",
    "        z = z_train_bad[S, 0:-1] # Store chosen prototypes\n",
    "    \n",
    "    eps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\n",
    "    feature_weight = np.zeros(z.shape)\n",
    "    \n",
    "    for i in range (z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            feature_weight[i, j] = np.exp(-1 * abs(Target[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n",
    "    \n",
    "    # move weights to a dataframe to display\n",
    "    \n",
    "    feature_weight_table = pd.DataFrame.from_records(np.around(feature_weight.astype('double'), 2))\n",
    "    feature_weight_table.columns = X_train.columns.tolist()\n",
    "    feature_weight_table = feature_weight_table.transpose() \n",
    "    return prototypes,feature_weight_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protodash_explain(X_train=X_train,X_test=X_test,model=xgboost_model,idx=8,m=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training data set and the predicted values of the model into the same array, \n",
    "# and distinguish the samples whose prediction results are good/bad\n",
    "\n",
    "pred_train = train_prediction.reshape((train_prediction.shape[0],1)) \n",
    "z_train = np.hstack((X_train_array, pred_train)) \n",
    "z_train_good = z_train[z_train[:,-1]==0, :]\n",
    "z_train_bad = z_train[z_train[:,-1]==1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomly select samples to explain from the test dataset\n",
    "idx = 9\n",
    "print(\"Chosen Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", xgboost_model.predict(X_test_array)[idx])\n",
    "print(\"Prediction probabilities:\", xgboost_model.predict_proba(X_test_array[idx].reshape(1,X_test_array.shape[1])))\n",
    "print(\"\")\n",
    "Target = np.hstack((X_test_array[idx].reshape((1,11)), xgboost_model.predict(X_test_array)[idx].reshape((1,1))))\n",
    "print(\"The values of the sample features to be explained are as follows：\")\n",
    "X_test.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Protodash interpreter\n",
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(Target, z_train_good, m=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = X_train.columns.tolist()\n",
    "col.append('RiskPerformance')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Display the value of each prototype feature and the corresponding Weight\n",
    "prototypes = pd.DataFrame(z_train_good[S, :])\n",
    "prototypes.columns = col\n",
    "prototypes[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5))\n",
    "prototypes.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Weight of each feature of each prototype\n",
    "\n",
    "z = z_train_good[S, 0:-1] # Store chosen prototypes\n",
    "eps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\n",
    "feature_weight = np.zeros(z.shape)\n",
    "for i in range (z.shape[0]):\n",
    "    for j in range(z.shape[1]):\n",
    "        feature_weight[i, j] = np.exp(-1 * abs(Target[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n",
    "                \n",
    "# move wts to a dataframe to display\n",
    "\n",
    "feature_weight_table = pd.DataFrame.from_records(np.around(feature_weight.astype('double'), 2))\n",
    "feature_weight_table.columns = X_train.columns.tolist()\n",
    "feature_weight_table.transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import lime\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,Y_test = train_test_split(X,Y,test_size=0.2, random_state=0)\n",
    "X_train_array = np.array(X_train)\n",
    "X_test_array = np.array(X_test)\n",
    "Y_train_array = np.array(Y_train)\n",
    "Y_test_array = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = xgboost.XGBClassifier(tree_method = 'hist',\n",
    "                         n_estimators = 800,\n",
    "                         min_child_weight = 6,\n",
    "                         max_depth = 2,\n",
    "                         gamma = 0,\n",
    "                         eta = 0.4,\n",
    "                         early_stop=10,\n",
    "                         random_state = 42)\n",
    "xgb_final.fit(X_train_array, Y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "def model_eval(model, title, test_features, test_labels):\n",
    "    scores = pd.DataFrame()\n",
    "    predictions = model.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels,predictions)\n",
    "    roc_auc = roc_auc_score(test_labels,predictions)\n",
    "    F1 = f1_score(test_labels,predictions)\n",
    "    precision = precision_score(test_labels,predictions)\n",
    "    recall = recall_score(test_labels,predictions)\n",
    "    scores[title] = [accuracy,roc_auc,F1,precision,recall]\n",
    "    scores.index = ['Accuracy Score', 'ROC_AUC', 'F1_Score', 'Precision_Score','Recall_Score']\n",
    "    return scores\n",
    "train_scores = model_eval(xgb_final,\"train\",X_train_array,Y_train_array)\n",
    "test_scores = model_eval(xgb_final, \"test\",X_test_array, Y_test_array)\n",
    "print(train_scores)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build an interpreter based on the XGBOOST model\n",
    "explainer = shap.TreeExplainer(xgb_final)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show global result attribution for multiple samples through thrust graph\n",
    "shap.force_plot(explainer.expected_value, shap_values[:100,:], X.iloc[:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display global result attribution through overview graph (bar)\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show global result attribution through overview graph (scatter)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"ExternalRiskEstimate\", shap_values, X)\n",
    "# Show attribution of results for a single feature through a dependency scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(explainer.expected_value, shap_values[:1],X.iloc[1])\n",
    "# Display the result attribution of a single sample through a decision road map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.decision_plot(explainer.expected_value,shap_interaction_values[:1],X.iloc[1],feature_display_range=slice(None, -20, -1))\n",
    "\n",
    "# Display the result attribution of a single sample (including the interaction between features) through a decision path diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=['ExternalRiskEstimate', 'MSinceOldestTradeOpen',\n",
    "       'MSinceMostRecentTradeOpen', 'AverageMInFile', 'NumSatisfactoryTrades',\n",
    "       'NumTrades60Ever2DerogPubRec', 'NumTrades90Ever2DerogPubRec',\n",
    "       'PercentTradesNeverDelq', 'MSinceMostRecentDelq',\n",
    "       'MaxDelq2PublicRecLast12M', 'MaxDelqEver', 'NumTotalTrades',\n",
    "       'NumTradesOpeninLast12M', 'PercentInstallTrades',\n",
    "       'MSinceMostRecentInqexcl7days', 'NumInqLast6M', 'NumInqLast6Mexcl7days',\n",
    "       'NetFractionRevolvingBurden', 'NetFractionInstallBurden',\n",
    "       'NumRevolvingTradesWBalance', 'NumInstallTradesWBalance',\n",
    "       'NumBank2NatlTradesWHighUtilization', 'PercentTradesWBalance']\n",
    "feature_names = X_test.columns.tolist()\n",
    "target_names=['Good','Bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=target_names, discretize_continuous=False)\n",
    "#Building LIME Interpreter Based on XGBOOST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, X_test.shape[0])\n",
    "print(i)\n",
    "X_test.shape[0]\n",
    "\n",
    "'''\n",
    "i = int(input('Please enter obbservation id: '))\n",
    "if i in str(X_test.shape[0]):\n",
    "    print('id exist')\n",
    "else:\n",
    "    i = int(input('Please enter obbservation id: '))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = lime_explainer.explain_instance(X_test.iloc[3], xgb_final.predict_proba, num_features=11)\n",
    "exp.show_in_notebook(show_table=True, show_all=False)\n",
    "# Explain attribution of 1-sample results with the LIME interpreter\n",
    "# Impression result attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to JSON\n",
    "#xgb_final.save_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_final, open('model2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
